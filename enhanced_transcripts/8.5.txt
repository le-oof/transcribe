Уважаемый слушатель, на данной лекции мы закончим рассмотрение философских проблем искусственного интеллекта. Используя понятия интенциональности, которые мы рассмотрели на прошлой лекции, обсудим вопросы синтаксиса и семантики искусственного интеллекта. Определим понятия. Синтаксис — это теория знаковой системы, определяющая правила использования знаков в том или ином языке. Семантика — это теория о значении знаков. Аргумент китайской комнаты Джона Сёрла утверждает, что язык искусственных систем не имеет семантики. Вся работа в системе интерфейса человек-компьютер со стороны машины происходит исключительно на синтаксическом уровне. Компьютер обучен определённым программам, алгоритмам связи символических элементов знаковой системы так, что возникает впечатление относительно их семантической нагруженности.

Возьмём в качестве примера работу географической электронной энциклопедии. Техническая система запрограммирована так, чтобы, получив от человека запрос «Как называется столица Непала?», выдать ответ «Катманду». При этом очевидно, что компьютер не понимает, что собственно стоит за теми знаками языка, которые использованы в данном запросе. Семантически они для него пусты. Просто в соответствующей программе дана директива: при запросе, представляющем собой один синтаксический комплекс, выдавать в качестве ответа другой. Машина может действовать как формальный логик. Отвлекаясь от какого-либо содержательного наполнения, она способна к оценке истинности сложных высказываний на основании анализа истинностных функций составляющих их простых высказываний. Однако эта дедуктивная ловкость машины будет только подтверждать тезис Джона Сёрла. В работе компьютера нас завораживает именно эта невероятная синтаксическая мощь, скорость оперирования знаками. Тем не менее, какими бы головокружительными ни были операции, связанные с синтаксисом формальных знаковых систем, компьютер никогда не сможет самостоятельно задать им какую-либо семантическую интерпретацию.

Аргумент китайской комнаты вызвал бурные обсуждения в рамках традиции философии искусственного интеллекта. Интересный поворот в исследовании данной проблемы, на который хотелось бы обратить внимание, заключается в том, что спустя десять лет тот же Джон Сёрл весьма оригинальным образом пересмотрел свой собственный аргумент. На этот раз вопрос был поставлен о синтаксисе: а можем ли мы утверждать, как делали ранее, что машина способна на выполнение синтаксических процедур в рамках заданной знаковой системы? Теперь американский философ дал отрицательный ответ и на этот вопрос.

Для того чтобы прояснить смысл сёрловской аргументации, вновь вспомним для начала англичанина, изучающего китайский. Классический аргумент относительно семантики начинался с того, что человек не понимает значения написанных на бумаге символов. Осваивая формальные правила операций с этими символами, он овладевает определённой синтаксической техникой, которая создает иллюзию семантической осведомлённости. Однако не пропустили ли мы здесь при описании данного лингвистического действия одного важного момента, на который нам следовало бы обратить внимание ещё до начала формулировки критического аргумента относительно семантики? Что именно может увидеть человек на предоставленных в его распоряжение листах бумаги? Строго говоря, на физическом уровне на листе бумаги виден лишь хаотический набор чернильных пятен различной формы. Получается, что прежде чем констатировать свою неосведомлённость относительно семантики языка, человек из китайской комнаты уже должен задать определённую синтаксическую интерпретацию. Он должен понять эти чернильные пятна на листе бумаги именно как знаки, которые, возможно, объединены какой-либо системой правил функционирования, составляя при этом единое целое — язык.

На одной из прошлых лекций мы уже обсудили вопрос о том, что в основании информатики была положена, по сути, простая идея объединения математической логики и электричества. К тому времени, к 30-м годам XX века, в логике уже прочно зарекомендовал себя новый подход, основанный на слиянии формально-логической символики и языка математики. Идея информатики заключалась в том, что было предложено интерпретировать наличие или отсутствие напряжения в электрической цепи как знаки математических символов, а единица и ноль им соответствовали. Так возник первый компьютер. Но составляют ли математические символы и единицы и нули синтаксис машины? То есть действительно ли «железо» компьютера — это нагромождение нулей и единиц? Джон Сёрл даёт отрицательный ответ на этот вопрос. Дело в том, что физика не имеет к синтаксису вообще никакого отношения. Наличие напряжения в электрической цепи — это ещё не «единица». Это лишь наличие напряжения в электрической цепи.

Представим ситуацию: мы вкручиваем лампочку в патрон настольной лампы с плохим качеством контакта проводников тока. Лампочка то загорается, то гаснет. Будем ли мы считать эти события на физическом уровне передачей какого-либо зашифрованного кода? Конечно же, нет. В этом как раз и состоит первичный интерпретативный шаг — понять определённый уровень электрического напряжения как знак. И пока не важно, знак чего — логического референта или, скажем, предупреждения об опасности пожара. Физический уровень в качестве материального носителя языковых выражений прежде семантической интерпретации должен сначала получить синтаксическую интерпретацию, которая задаётся внешним образом через пользователей данной знаковой системы.

Если при использовании электронной энциклопедии я задаю вопрос о столице Непала и получаю на него ответ, то машина не только не понимает значения символов, с которыми она оперирует согласно определённому алгоритму, но и не представляет собой даже формальную синтаксическую систему. На физическом уровне вслед за одним случаем высокого напряжения в определённом участке цепи возникает другой случай — только и всего. Для того чтобы эти факты высокого напряжения понять как знаки, которые могут подчиняться определённым операциональным правилам их сочетания, необходимо задать первичную синтаксическую интерпретацию, на которую в дальнейшем будет опираться программист при формулировке соответствующего алгоритма операции.

На основании изложенного материала последних трёх лекций мы можем заключить, что невозможно получить однозначный ответ на вопрос о том, являются ли существующие на сегодняшний день технические системы действительно интеллектуальными в смысле сильного искусственного интеллекта. Ответ на данный вопрос будет зависеть от системы отсчёта, в которой мы работаем, то есть от той парадигмы понимания интеллектуальной деятельности, которую мы выбираем.

В этих условиях весьма полезным ввиду своей простоты выступает так называемый тест Тьюринга на определение интеллектуальности технической системы. Обсуждением этой разработки закончим наши рассмотрения философских проблем искусственного интеллекта. Алан Тьюринг — английский математик и инженер, один из основоположников информатики, стоял у истоков теории и практического развития компьютерной техники.

С точки зрения теста Тьюринга, мы должны исходить из бихевиористских позиций при исследовании интеллекта. Бихевиоризм (от англ. behaviour — поведение) — направление в психологии, изучающее мышление с точки зрения поведения, то есть реакции человека. Бихевиоризм рассматривает сознание в качестве так называемого «чёрного ящика»: мы знаем, что условно подаётся на вход этого устройства, и мы знаем, что имеем на выходе, но просто перестаём задавать вопрос о том, что происходит внутри него. Тем самым все сложные вопросы с определением того, как именно на физическом и на психическом уровне функционирует человеческий разум, оказываются вне рассмотрения. Мы обсуждаем только внешнее проявление человека: задаём ему вопросы и фиксируем его реакцию, то есть слушаем ответы, наблюдаем за его действиями.

Точно так же Тьюринг предложил оценивать и работу системы искусственного интеллекта. Главный тезис его теста гласит следующее: если по внешним признакам своей деятельности машина демонстрирует сходство с деятельностью человека, то, вне зависимости от того, что происходит внутри машины, будем считать её поведение разумным. Проще говоря, если ответы машины на поставленные вопросы невозможно отличить от ответов, которые на эти же вопросы дал бы человек, то следует считать, что машина проходит тест на интеллектуальность. Например, если на вопрос «Сколько будет 2 плюс 2?» машина выдаёт ответ «4», будем считать это разумным ответом. Очевидно, что при решении арифметических задач электронно-вычислительные машины легко проходят тест Тьюринга. Однако при постановке более сложных вопросов, где чёткой алгоритмизации добиться намного труднее, прохождение теста Тьюринга — не простая задача.